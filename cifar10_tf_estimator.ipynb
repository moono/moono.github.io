{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar 10 loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################\n",
    "# cifar10 data file loader\n",
    "# #########################\n",
    "def _unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        item_dict = pickle.load(fo, encoding='bytes')\n",
    "    return item_dict\n",
    "\n",
    "\n",
    "def _load_n_preprocess(file_name):\n",
    "    batch_loaded = _unpickle(file_name)\n",
    "    labels = batch_loaded[b'labels']\n",
    "    images = batch_loaded[b'data']\n",
    "\n",
    "    # change type & reshape\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "    images = np.reshape(images, newshape=(-1, 3, 32, 32))\n",
    "    images = np.transpose(images, axes=(0, 2, 3, 1))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_cifar10(cifar10_data_path):\n",
    "    # download from: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    # place where data batches resides\n",
    "    dataset_path = os.path.join(cifar10_data_path, 'cifar-10-batches-py')\n",
    "\n",
    "    # parse training batch files and concat\n",
    "    training_batch_images = None\n",
    "    training_batch_labels = None\n",
    "    for ii in range(1, 6):\n",
    "        file_name = os.path.join(dataset_path, 'data_batch_{:d}'.format(ii))\n",
    "        images, labels = _load_n_preprocess(file_name)\n",
    "\n",
    "        if training_batch_images is None:\n",
    "            training_batch_images = images\n",
    "            training_batch_labels = labels\n",
    "        else:\n",
    "            training_batch_images = np.concatenate((training_batch_images, images), axis=0)\n",
    "            training_batch_labels = np.concatenate((training_batch_labels, labels), axis=0)\n",
    "\n",
    "    # parse test batch file\n",
    "    file_name = os.path.join(dataset_path, 'test_batch')\n",
    "    test_batch_images, test_batch_labels = _load_n_preprocess(file_name)\n",
    "    return training_batch_images, training_batch_labels, test_batch_images, test_batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################\n",
    "# Tensorflow dataset api helpers\n",
    "# ###############################\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "\n",
    "def make_generator(images, labels):\n",
    "    def _generator():\n",
    "        for image, label in zip(images, labels):\n",
    "            yield image, label\n",
    "    return _generator\n",
    "\n",
    "\n",
    "def parse_fn(images, labels):\n",
    "    # convert to float32 and rescale images to -1.0 ~ 1.0\n",
    "    images = tf.image.convert_image_dtype(images, dtype=tf.float32)\n",
    "    images = tf.multiply(tf.subtract(images, 0.5), 2.0)\n",
    "    images.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def preprocess_fn(image, label, is_training):\n",
    "    if is_training:\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.clip_by_value(image, -1.0, 1.0)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def graph_input_fn(cifar10_images, cifar10_labels, batch_size, is_training):\n",
    "    dataset = tf.data.Dataset.from_generator(make_generator(cifar10_images, cifar10_labels), (tf.uint8, tf.int32))\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.map(parse_fn)\n",
    "    dataset = dataset.map(lambda image, label: preprocess_fn(image, label, is_training))\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    features = {\n",
    "        'x': images,\n",
    "    }\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck residual unit with 3 sub layers with preactivation\n",
    "def bottleneck_residual_block_v2(x, in_filter, out_filter, stage, block, is_training, strides, scale=0.0):\n",
    "    l2_regularizer = tf.contrib.layers.l2_regularizer(scale=scale)\n",
    "    conv_name = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name = 'bn' + str(stage) + block + '_branch'\n",
    "    bottleneck_filter = out_filter // 4\n",
    "\n",
    "    # conv 1x1\n",
    "    x = tf.layers.batch_normalization(x, training=is_training, name=bn_name + '2a')\n",
    "    x = tf.nn.relu(x)\n",
    "    original_x = x\n",
    "    x = tf.layers.conv2d(x, bottleneck_filter, 1, strides, padding='same', use_bias=False,\n",
    "                         kernel_regularizer=l2_regularizer, name=conv_name + '2a')\n",
    "\n",
    "    # conv 3x3\n",
    "    x = tf.layers.batch_normalization(x, training=is_training, name=bn_name + '2b')\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x, bottleneck_filter, 3, 1, padding='same', use_bias=False,\n",
    "                         kernel_regularizer=l2_regularizer, name=conv_name + '2b')\n",
    "\n",
    "    # conv 1x1\n",
    "    x = tf.layers.batch_normalization(x, training=is_training, name=bn_name + '2c')\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x, out_filter, 1, 1, padding='same', use_bias=False,\n",
    "                         kernel_regularizer=l2_regularizer, name=conv_name + '2c')\n",
    "\n",
    "    # match spatial dimension with conv2d\n",
    "    if in_filter != out_filter:\n",
    "        original_x = tf.layers.conv2d(original_x, out_filter, 1, strides, padding='same', use_bias=False,\n",
    "                                      kernel_regularizer=l2_regularizer, name=conv_name + '1')\n",
    "\n",
    "    x = tf.add(x, original_x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_cifar10(images, n_classes, is_training, weight_decay):\n",
    "    n_layers = 9\n",
    "\n",
    "    # stage 1: [32, 32, 16]\n",
    "    x = tf.layers.conv2d(images, filters=16, kernel_size=3, strides=1, padding='same', use_bias=False,\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "\n",
    "    # stage 2: [32, 32, 64]\n",
    "    stage = 2\n",
    "    block = chr(ord('a'))\n",
    "    x = bottleneck_residual_block_v2(x, 16, 64, stage, block, is_training, strides=1, scale=weight_decay)\n",
    "    for ii in range(1, n_layers):\n",
    "        block = chr(ord(block) + 1)\n",
    "        x = bottleneck_residual_block_v2(x, 64, 64, stage, block, is_training, strides=1, scale=weight_decay)\n",
    "\n",
    "    # stage 3: [16, 16, 128]\n",
    "    stage = 3\n",
    "    block = chr(ord('a'))\n",
    "    x = bottleneck_residual_block_v2(x, 64, 128, stage, block, is_training, strides=2, scale=weight_decay)\n",
    "    for ii in range(1, n_layers):\n",
    "        block = chr(ord(block) + 1)\n",
    "        x = bottleneck_residual_block_v2(x, 128, 128, stage, block, is_training, strides=1, scale=weight_decay)\n",
    "\n",
    "    # stage 4: [8, 8, 256]\n",
    "    stage = 4\n",
    "    block = chr(ord('a'))\n",
    "    x = bottleneck_residual_block_v2(x, 128, 256, stage, block, is_training, strides=2, scale=weight_decay)\n",
    "    for ii in range(1, n_layers):\n",
    "        block = chr(ord(block) + 1)\n",
    "        x = bottleneck_residual_block_v2(x, 256, 256, stage, block, is_training, strides=1, scale=weight_decay)\n",
    "\n",
    "    # global average pooling\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.reduce_mean(x, axis=[1, 2])\n",
    "\n",
    "    # fully connected layer\n",
    "    logits = tf.layers.dense(x, units=n_classes, kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # parse parameters\n",
    "    learning_rate = params['learning_rate']\n",
    "    weight_decay = params['weight_decay']\n",
    "    momentum = params['momentum']\n",
    "    n_classes = params['n_classes']\n",
    "    input_size = params['input_size']\n",
    "\n",
    "    images = tf.reshape(features['x'], shape=[-1, *input_size])\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # build networks\n",
    "    logits = resnet_cifar10(images, n_classes, is_training, weight_decay)\n",
    "\n",
    "    # ================================\n",
    "    # prediction mode\n",
    "    # ================================\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_id': tf.argmax(logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # compute loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    loss += tf.losses.get_regularization_loss()\n",
    "\n",
    "    # compute evaluation metric\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1), name='acc_op')\n",
    "    tf.summary.scalar('accuracy', accuracy[1])  # during training\n",
    "\n",
    "    # ================================\n",
    "    # evaluation mode\n",
    "    # ================================\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'accuracy': accuracy})\n",
    "\n",
    "    # ================================\n",
    "    # training mode\n",
    "    # ================================\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    # prepare optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    elif params['optimizer'] == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_ops = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# grab data\n",
    "cifar10_data_path = 'D:\\\\db'\n",
    "train_x, train_y, test_x, test_y = load_cifar10(cifar10_data_path)\n",
    "input_size = train_x.shape[1:]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models\\ResNet_CIFAR10\n"
     ]
    }
   ],
   "source": [
    "# set model path\n",
    "model_name = 'ResNet_CIFAR10'\n",
    "models_dir = os.path.join('./models', model_name)\n",
    "print(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_task_type': 'worker', '_task_id': 0, '_eval_distribute': None, '_protocol': None, '_train_distribute': None, '_model_dir': './models\\\\ResNet_CIFAR10', '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_global_id_in_cluster': 0, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016291566E48>, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_experimental_distribute': None, '_tf_random_seed': None, '_evaluation_master': '', '_device_fn': None, '_master': '', '_save_summary_steps': 100, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "# create the Estimator\n",
    "model = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=models_dir,\n",
    "    config=None,\n",
    "    params={\n",
    "        'input_size': test_x.shape[1:],\n",
    "        'n_classes': 10,\n",
    "        'optimizer': 'momentum',\n",
    "        'learning_rate': 0.1,\n",
    "        'weight_decay': 2e-4,\n",
    "        'momentum': 0.9,\n",
    "    },\n",
    "    warm_start_from=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./models\\ResNet_CIFAR10\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.1705375, step = 0\n",
      "INFO:tensorflow:global_step/sec: 6.36865\n",
      "INFO:tensorflow:loss = 2.3620603, step = 100 (15.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7911\n",
      "INFO:tensorflow:loss = 2.1439214, step = 200 (9.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7422\n",
      "INFO:tensorflow:loss = 2.0121799, step = 300 (9.259 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 391 into ./models\\ResNet_CIFAR10\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.9359593.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-02-05:30:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./models\\ResNet_CIFAR10\\model.ckpt-391\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-02-05:30:18\n",
      "INFO:tensorflow:Saving dict for global step 391: accuracy = 0.3972, global_step = 391, loss = 2.095225\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 391: ./models\\ResNet_CIFAR10\\model.ckpt-391\n",
      "{'global_step': 391, 'loss': 2.095225, 'accuracy': 0.3972}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./models\\ResNet_CIFAR10\\model.ckpt-391\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 391 into ./models\\ResNet_CIFAR10\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.1493242, step = 391\n",
      "INFO:tensorflow:global_step/sec: 6.52034\n",
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.51589644, step = 36754\n",
      "INFO:tensorflow:global_step/sec: 6.49953\n",
      "INFO:tensorflow:loss = 0.623862, step = 36854 (15.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9573\n",
      "INFO:tensorflow:loss = 0.5610008, step = 39009 (9.361 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 39100 into ./models\\ResNet_CIFAR10\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.48679906.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-02-07:32:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./models\\ResNet_CIFAR10\\model.ckpt-39100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-02-07:32:16\n",
      "INFO:tensorflow:Saving dict for global step 39100: accuracy = 0.8524, global_step = 39100, loss = 0.87268287\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 39100: ./models\\ResNet_CIFAR10\\model.ckpt-39100\n",
      "{'global_step': 39100, 'loss': 0.87268287, 'accuracy': 0.8524}\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "for e in range(0, epochs):\n",
    "    model.train(input_fn=lambda: graph_input_fn(train_x, train_y, batch_size=batch_size, is_training=True))\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        # evaluate\n",
    "        eval_results = model.evaluate(input_fn=lambda: graph_input_fn(test_x, test_y, batch_size=100, is_training=False))\n",
    "        print(eval_results)\n",
    "\n",
    "# final evaluation\n",
    "eval_results = model.evaluate(input_fn=lambda: graph_input_fn(test_x, test_y, batch_size=100, is_training=False))\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
